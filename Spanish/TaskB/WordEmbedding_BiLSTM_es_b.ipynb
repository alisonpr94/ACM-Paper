{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "WordEmbedding_BiLSTM_es_b.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "QL28WSHoBsfp",
        "colab_type": "code",
        "outputId": "a19f5876-8dbf-4199-c2aa-f564df8f96cc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\n",
        "#@author: alison\n",
        "\n",
        "import re\n",
        "import string\n",
        "import pickle\n",
        "import keras\n",
        "import nltk\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer, SnowballStemmer\n",
        "from nltk.tokenize import TweetTokenizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing import sequence\n",
        "from keras.models import Model\n",
        "from keras.layers import Dense, Dropout, Embedding, LSTM, CuDNNLSTM\n",
        "from keras.layers import Input, Activation, Bidirectional\n",
        "from keras.optimizers import Adam, SGD, RMSprop\n",
        "from keras import optimizers\n",
        "from keras import regularizers"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "XdekfYsoV2c2",
        "colab_type": "code",
        "outputId": "88b78a28-5240-454f-cb19-5feb2f871403",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        }
      },
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "metadata": {
        "id": "-TpI0dMJEi6p",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!pip install -U -q PyDrive\n",
        " \n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        " \n",
        "# 1. Authenticate and create the PyDrive client.\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "XEfvBz8IEqrO",
        "colab_type": "code",
        "outputId": "b79fec93-00c6-41d6-9091-7f9df6f0b008",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 372
        }
      },
      "cell_type": "code",
      "source": [
        "file_list = drive.ListFile({'q': \"'1JsHac7_mxx_M8BapYSzcDKnzl4M9e1Yb' in parents and trashed=false\"}).GetList()\n",
        "for file1 in file_list:\n",
        "    print('title: %s, id: %s' % (file1['title'], file1['id']))"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "title: WordEmbedding_BiLSTM_es_b.ipynb, id: 1RsRiCK1r31zWBCi_otXDYl_sCkXPBXgS\n",
            "title: WordEmbedding_LSTM_es_b.ipynb, id: 1X0gbSY2GSaAFAux8TtspIXHPYefVE01H\n",
            "title: CNN_es_b.ipynb, id: 1DRMuNX3zGuwQ8t1G5SCKroPzVv76PL8T\n",
            "title: WordEmbedding_GRU_en_b.ipynb, id: 107hV4YXnoqImUVPmP4Sa7l1n_XDc0Leo\n",
            "title: WordEmbedding_BiLSTM_en_b.ipynb, id: 1J3ZyxVQedp3nJ-cXnDl4RcERkCBpa_D1\n",
            "title: WordEmbedding_LSTM_en_b.ipynb, id: 1YoA0jnTr0fvUVRICR25-Y0ygX5s2DeUS\n",
            "title: WordEmbedding_GRU_es_b.ipynb, id: 1f87RaAgRU_RYsrSXZoHMu7ylpMv__0IV\n",
            "title: bi_lstm_es_b.ipynb, id: 15LbftMx8bfHgl4In11dl2ESceanBnZgx\n",
            "title: lstm_es_b.ipynb, id: 1zne6axRA92wAUbRXh3I0QAZ17l_-pY-Q\n",
            "title: gru_es_b.ipynb, id: 1YPRlzJgn04aMlY9kbU6NNgFdHYfQhmwb\n",
            "title: CNN_en_b.ipynb, id: 1Y4-iz6jsFtExSbQu4Iw0Rdwz23cAcZA2\n",
            "title: bi_lstm_en_b.ipynb, id: 19N0cZ6WKYpnTfDVgKn1i-4UzLEN4jmSK\n",
            "title: lstm_en_b.ipynb, id: 1n63dDEnPeqMTASzhwnaDnG_8knVqmIkX\n",
            "title: gru_en_b.ipynb, id: 1l3cd4flv4i1iOwKjLdnDb5082nNkqvYe\n",
            "title: test_en.tsv, id: 1Z-kJ95PF2VZiGAn_5Piyovk8SZ-bYQJv\n",
            "title: test_es.tsv, id: 1PlU_aDjY9RLq2CKW1zMQTDmRDcZ0f8YL\n",
            "title: train_en.tsv, id: 1tv4kRZtCJu7F4WtkBvaMWZMXZZDYF-yv\n",
            "title: dev_en.tsv, id: 116R1Q_P_m75ZagpkE7cZXfXHiDSDwrJy\n",
            "title: train_es.tsv, id: 18SENOaqh8YZ98vrzHgHP3sQk3sycMz0H\n",
            "title: dev_es.tsv, id: 1S5cZDZq7Mhmxpp_SUs79j86An3ICs265\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "h2hLjMJyE3dR",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "train_downloaded = drive.CreateFile({'id': '18SENOaqh8YZ98vrzHgHP3sQk3sycMz0H'})\n",
        "train_downloaded.GetContentFile('train_es.tsv')\n",
        "dev_downloaded = drive.CreateFile({'id': '1S5cZDZq7Mhmxpp_SUs79j86An3ICs265'})\n",
        "dev_downloaded.GetContentFile('dev_es.tsv')\n",
        "test_downloaded = drive.CreateFile({'id': '1PlU_aDjY9RLq2CKW1zMQTDmRDcZ0f8YL'})\n",
        "test_downloaded.GetContentFile('test_es.tsv')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "0s3Dnnk8FF4j",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "train = pd.read_csv('train_es.tsv', delimiter='\\t',encoding='utf-8')\n",
        "dev = pd.read_csv('dev_es.tsv', delimiter='\\t',encoding='utf-8')\n",
        "#test = pd.read_csv('test_es.tsv', delimiter='\\t',encoding='utf-8')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "lrslbThJCeSY",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Etapa de prÃ©-processamento\n",
        "\n",
        "def clean_tweets(tweet):\n",
        "    tweet = re.sub('@(\\\\w{1,15})\\b', '', tweet)\n",
        "    tweet = tweet.replace(\"via \", \"\")\n",
        "    tweet = tweet.replace(\"RT \", \"\")\n",
        "    tweet = tweet.lower()\n",
        "    return tweet\n",
        "    \n",
        "def clean_url(tweet):\n",
        "    tweet = re.sub('http\\\\S+', '', tweet, flags=re.MULTILINE)   \n",
        "    return tweet\n",
        "    \n",
        "def remove_stop_words(tweet):\n",
        "    stops = set(stopwords.words(\"spanish\"))\n",
        "    stops.update(['.',',','\"',\"'\",'?',':',';','(',')','[',']','{','}'])\n",
        "    toks = [tok for tok in tweet if not tok in stops and len(tok) >= 3]\n",
        "    return toks\n",
        "    \n",
        "def stemming_tweets(tweet):\n",
        "    stemmer = SnowballStemmer('spanish')\n",
        "    stemmed_words = [stemmer.stem(word) for word in tweet]\n",
        "    return stemmed_words\n",
        "\n",
        "def remove_number(tweet):\n",
        "    newTweet = re.sub('\\\\d+', '', tweet)\n",
        "    return newTweet\n",
        "\n",
        "def remove_hashtags(tweet):\n",
        "    result = ''\n",
        "\n",
        "    for word in tweet.split():\n",
        "        if word.startswith('#') or word.startswith('@'):\n",
        "            result += word[1:]\n",
        "            result += ' '\n",
        "        else:\n",
        "            result += word\n",
        "            result += ' '\n",
        "\n",
        "    return result"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "QDsnLDRQGl71",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def preprocessing(tweet, swords = True, url = True, stemming = True, ctweets = True, number = True, hashtag = True):\n",
        "\n",
        "    if ctweets:\n",
        "        tweet = clean_tweets(tweet)\n",
        "\n",
        "    if url:\n",
        "        tweet = clean_url(tweet)\n",
        "\n",
        "    if hashtag:\n",
        "        tweet = remove_hashtags(tweet)\n",
        "    \n",
        "    twtk = TweetTokenizer(strip_handles=True, reduce_len=True)\n",
        "\n",
        "    if number:\n",
        "        tweet = remove_number(tweet)\n",
        "    \n",
        "    tokens = [w.lower() for w in twtk.tokenize(tweet) if w != \"\" and w is not None]\n",
        "\n",
        "    if swords:\n",
        "        tokens = remove_stop_words(tokens)\n",
        "\n",
        "    if stemming:\n",
        "        tokens = stemming_tweets(tokens)\n",
        "\n",
        "    text = \" \".join(tokens)\n",
        "\n",
        "    return text"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "SunHFjyyFLR3",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "train_text  = train['text'].map(lambda x: preprocessing(x, swords = True, url = True, stemming = True, ctweets = True, number = True, hashtag = True))\n",
        "hs_train    = train['HS']\n",
        "id_train    = train['id']\n",
        "tr_train    = train['TR']\n",
        "ag_train    = [1 if k == '1' else 0 for k in train['AG']]\n",
        "\n",
        "test_text  = dev['text'].map(lambda x: preprocessing(x, swords = True, url = True, stemming = True, ctweets = True, number = True, hashtag = True))\n",
        "hs_test    = dev['HS']\n",
        "id_test    = dev['id']\n",
        "tr_test    = dev['TR']\n",
        "ag_test    = dev['AG']\n",
        "\n",
        "max_features = 25000\n",
        "maxlen = 100\n",
        "word_embedding_dim = 300"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "zdMmsNxqM8S4",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def code_data(train_text, test_text, maxlen, max_features):\n",
        "    data = np.concatenate((train_text, test_text), axis=0)\n",
        "\n",
        "    # Treina um tokenizaddor nos dados de treino\n",
        "    tokenizer = Tokenizer(num_words=max_features)\n",
        "    tokenizer.fit_on_texts(data)\n",
        "\n",
        "    # Tokeniza os dados\n",
        "    X = tokenizer.texts_to_sequences(data)\n",
        "    Y = tokenizer.texts_to_sequences(test_text)\n",
        "\n",
        "    tweets = sequence.pad_sequences(X, maxlen=maxlen)\n",
        "    x_test = sequence.pad_sequences(Y, maxlen=maxlen)\n",
        "    \n",
        "    word_index = tokenizer.word_index\n",
        "    \n",
        "    return tweets, x_test, word_index"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "pTZGv8lcWGIU",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "tweets, x_test, word_index = code_data(train_text, test_text, maxlen, max_features)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "OPki5CvtNG-i",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "num_words = min(max_features, len(word_index) + 1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ruveMxQ0NJ4F",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "_lFa0GNANKaw",
        "colab_type": "code",
        "outputId": "357b5454-a873-4dea-b063-a8fbdfcc3e8e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 141
        }
      },
      "cell_type": "code",
      "source": [
        "file_list = drive.ListFile({'q': \"'1sBKK1i4JXIluelnAPqjOU4xVIIPA3Vmy' in parents and trashed=false\"}).GetList()\n",
        "for file1 in file_list:\n",
        "    print('title: %s, id: %s' % (file1['title'], file1['id']))"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "title: GoogleNews-word2vec-300_en.bin, id: 1bERzzLRCcPdavQYVJCwLvt8uWmiRdg_o\n",
            "title: glove-300d_es.vec, id: 167op9mvIKVfH1XGm3dkZ7afHReiqvmIN\n",
            "title: glove.6B.300d_en.txt, id: 1FRfAM3GouOxelBo_gwj5n4YNWDMP6CkL\n",
            "title: wiki_fasttext_es.vec, id: 1S1ObfL0R15IzRK_T1RrSxZMn0TJDECsN\n",
            "title: wiki_fasttext_en.vec, id: 18zPA_9tWmlNZChdfMKHTp0Ka1SQsnkj9\n",
            "title: TaskA, id: 1Hx5OP1Yrlh37yYzSMtsv6Ui_fuzOuG04\n",
            "title: TaskB, id: 1JsHac7_mxx_M8BapYSzcDKnzl4M9e1Yb\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "cwYMHLiwOLvO",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "we    = {'id': '167op9mvIKVfH1XGm3dkZ7afHReiqvmIN', 'file': 'glove-300d_es.vec'}\n",
        "#we = {'id': '1S1ObfL0R15IzRK_T1RrSxZMn0TJDECsN', 'file': 'wiki_fasttext_es.vec'}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "x2WBRPz3NRLk",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "word_vector = drive.CreateFile({'id': we['id']})\n",
        "word_vector.GetContentFile(we['file'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "PL3T-Qg2NUM2",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def word_embeddings(word_index, num_words, word_embedding_dim):\n",
        "    embeddings_index = {}\n",
        "    \n",
        "    f = open(we['file'], 'r', encoding='utf-8')\n",
        "    \n",
        "    for line in tqdm(f):\n",
        "        values = line.rstrip().rsplit(' ')\n",
        "        word = values[0]\n",
        "        coefs = np.asarray(values[1:], dtype='float32')\n",
        "        embeddings_index[word] = coefs\n",
        "    \n",
        "    f.close()\n",
        "\n",
        "    embedding_matrix = np.zeros((num_words, word_embedding_dim))\n",
        "    \n",
        "    for word, i in word_index.items():\n",
        "        if i >= max_features:\n",
        "            continue\n",
        "        \n",
        "        embedding_vector = embeddings_index.get(word)\n",
        "        \n",
        "        if embedding_vector is not None:\n",
        "            embedding_matrix[i] = embedding_vector\n",
        "\n",
        "    return embedding_matrix"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "TkJ_dzMrNYaa",
        "colab_type": "code",
        "outputId": "90e4fc5a-7002-4bd2-c32a-af14167d548e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "weights_embedding_matrix = word_embeddings(word_index, num_words, word_embedding_dim)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "855381it [01:15, 11295.60it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "BawjLqxwNcwS",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Fase de classificaÃ§Ã£o de sentimentos\n",
        "\n",
        "epochs = 5\n",
        "batch_size = 32\n",
        "\n",
        "tweet_input = Input(shape=(maxlen,), dtype='int32')\n",
        "\n",
        "embedding = Embedding(num_words, word_embedding_dim, input_length=maxlen, weights=[weights_embedding_matrix], trainable=True)(tweet_input)\n",
        "\n",
        "gru1 = Bidirectional(CuDNNLSTM(256, kernel_initializer='normal', return_sequences=True))(embedding)\n",
        "\n",
        "gru2 = Bidirectional(CuDNNLSTM(256))(gru1)\n",
        "\n",
        "dens = Dense(256, activation='relu')(gru2)\n",
        "\n",
        "output = Dense(1, activation='sigmoid')(dens)\n",
        "\n",
        "model = Model(inputs=tweet_input, outputs=output)\n",
        "\n",
        "#opt = Adam(lr=0.001, beta_1=0.9, beta_2=0.825, epsilon=1e-08)\n",
        "#opt = SGD(lr=0.01, momentum=0.0, decay=0.0, nesterov=False)\n",
        "opt = RMSprop(lr=0.001, rho=0.9, epsilon=None, decay=0.01)\n",
        "\n",
        "model.compile(optimizer=opt, loss='binary_crossentropy', metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "2IjUtKhIIKQO",
        "colab_type": "code",
        "outputId": "367b8609-ee3e-4564-f0ba-396c89b4c329",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 550
        }
      },
      "cell_type": "code",
      "source": [
        "# Treinamento e prediÃ§Ã£o - Hate Speech (HS)\n",
        "\n",
        "classes = np.concatenate((hs_train, hs_test), axis=0)\n",
        "\n",
        "x_train, x_val, y_train, y_val = train_test_split(tweets, classes, test_size=0.25, random_state=None)\n",
        "\n",
        "model.summary()\n",
        "\n",
        "model.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, shuffle=True, validation_data=(x_val, y_val), validation_steps=None)\n",
        "\n",
        "hs = (model.predict(x_test, batch_size=batch_size) > .5).astype(int)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_1 (InputLayer)         (None, 100)               0         \n",
            "_________________________________________________________________\n",
            "embedding_1 (Embedding)      (None, 100, 300)          3513900   \n",
            "_________________________________________________________________\n",
            "bidirectional_1 (Bidirection (None, 100, 512)          1142784   \n",
            "_________________________________________________________________\n",
            "bidirectional_2 (Bidirection (None, 512)               1576960   \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 256)               131328    \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 1)                 257       \n",
            "=================================================================\n",
            "Total params: 6,365,229\n",
            "Trainable params: 6,365,229\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Train on 3726 samples, validate on 1243 samples\n",
            "Epoch 1/5\n",
            "3726/3726 [==============================] - 19s 5ms/step - loss: 0.6011 - acc: 0.6873 - val_loss: 0.5672 - val_acc: 0.7160\n",
            "Epoch 2/5\n",
            "3726/3726 [==============================] - 17s 4ms/step - loss: 0.4263 - acc: 0.8100 - val_loss: 0.5876 - val_acc: 0.7353\n",
            "Epoch 3/5\n",
            "3726/3726 [==============================] - 17s 4ms/step - loss: 0.3200 - acc: 0.8693 - val_loss: 0.5860 - val_acc: 0.7393\n",
            "Epoch 4/5\n",
            "3726/3726 [==============================] - 17s 4ms/step - loss: 0.2383 - acc: 0.9047 - val_loss: 0.7510 - val_acc: 0.7208\n",
            "Epoch 5/5\n",
            "3726/3726 [==============================] - 17s 4ms/step - loss: 0.1726 - acc: 0.9321 - val_loss: 1.0188 - val_acc: 0.7080\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "rx8QHLaNWgdn",
        "colab_type": "code",
        "outputId": "96567a92-a9f3-408c-fc07-41e51d6ba27e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 550
        }
      },
      "cell_type": "code",
      "source": [
        "# Treinamento e prediÃ§Ã£o - Target Range (TR)\n",
        "\n",
        "classes = np.concatenate((tr_train, tr_test), axis=0)\n",
        "\n",
        "x_train, x_val, y_train, y_val = train_test_split(tweets, classes, test_size=0.25, random_state=None)\n",
        "\n",
        "model.summary()\n",
        "\n",
        "model.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, shuffle=True, validation_data=(x_val, y_val), validation_steps=None)\n",
        "\n",
        "tr = (model.predict(x_test, batch_size=batch_size) > .5).astype(int)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_1 (InputLayer)         (None, 100)               0         \n",
            "_________________________________________________________________\n",
            "embedding_1 (Embedding)      (None, 100, 300)          3513900   \n",
            "_________________________________________________________________\n",
            "bidirectional_1 (Bidirection (None, 100, 512)          1142784   \n",
            "_________________________________________________________________\n",
            "bidirectional_2 (Bidirection (None, 512)               1576960   \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 256)               131328    \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 1)                 257       \n",
            "=================================================================\n",
            "Total params: 6,365,229\n",
            "Trainable params: 6,365,229\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Train on 3726 samples, validate on 1243 samples\n",
            "Epoch 1/5\n",
            "3726/3726 [==============================] - 17s 4ms/step - loss: 0.3320 - acc: 0.8583 - val_loss: 0.2905 - val_acc: 0.8850\n",
            "Epoch 2/5\n",
            "3726/3726 [==============================] - 17s 4ms/step - loss: 0.2322 - acc: 0.9023 - val_loss: 0.3120 - val_acc: 0.8793\n",
            "Epoch 3/5\n",
            "3726/3726 [==============================] - 17s 4ms/step - loss: 0.1868 - acc: 0.9240 - val_loss: 0.3076 - val_acc: 0.8930\n",
            "Epoch 4/5\n",
            "3726/3726 [==============================] - 17s 4ms/step - loss: 0.1517 - acc: 0.9353 - val_loss: 0.3262 - val_acc: 0.8850\n",
            "Epoch 5/5\n",
            "3726/3726 [==============================] - 17s 4ms/step - loss: 0.1228 - acc: 0.9503 - val_loss: 0.3621 - val_acc: 0.8858\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "n4b5VACKIw8w",
        "colab_type": "code",
        "outputId": "2c7934b8-936b-4c9f-8f66-90d6c730ab64",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 550
        }
      },
      "cell_type": "code",
      "source": [
        "# Treinamento e prediÃ§Ã£o - Aggressiveness (AG)\n",
        "\n",
        "classes = np.concatenate((ag_train, ag_test), axis=0)\n",
        "\n",
        "x_train, x_val, y_train, y_val = train_test_split(tweets, classes, test_size=0.25, random_state=None)\n",
        "\n",
        "model.summary()\n",
        "\n",
        "model.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, shuffle=True, validation_data=(x_val, y_val), validation_steps=None)\n",
        "\n",
        "ag = (model.predict(x_test, batch_size=batch_size) > .5).astype(int)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_1 (InputLayer)         (None, 100)               0         \n",
            "_________________________________________________________________\n",
            "embedding_1 (Embedding)      (None, 100, 300)          3513900   \n",
            "_________________________________________________________________\n",
            "bidirectional_1 (Bidirection (None, 100, 512)          1142784   \n",
            "_________________________________________________________________\n",
            "bidirectional_2 (Bidirection (None, 512)               1576960   \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 256)               131328    \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 1)                 257       \n",
            "=================================================================\n",
            "Total params: 6,365,229\n",
            "Trainable params: 6,365,229\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Train on 3726 samples, validate on 1243 samples\n",
            "Epoch 1/5\n",
            "3726/3726 [==============================] - 17s 4ms/step - loss: 0.1673 - acc: 0.9560 - val_loss: 0.1317 - val_acc: 0.9630\n",
            "Epoch 2/5\n",
            "3726/3726 [==============================] - 17s 4ms/step - loss: 0.1248 - acc: 0.9648 - val_loss: 0.1322 - val_acc: 0.9630\n",
            "Epoch 3/5\n",
            "3726/3726 [==============================] - 17s 4ms/step - loss: 0.1130 - acc: 0.9654 - val_loss: 0.1421 - val_acc: 0.9606\n",
            "Epoch 4/5\n",
            "3726/3726 [==============================] - 17s 4ms/step - loss: 0.1021 - acc: 0.9665 - val_loss: 0.1516 - val_acc: 0.9606\n",
            "Epoch 5/5\n",
            "3726/3726 [==============================] - 17s 4ms/step - loss: 0.0908 - acc: 0.9697 - val_loss: 0.1714 - val_acc: 0.9574\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "4DpgoxpT88nK",
        "colab_type": "code",
        "outputId": "89f1607a-2369-408c-b974-1f2667b68ff6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "def func(pred):\n",
        "    output = []\n",
        "    \n",
        "    for array in pred:\n",
        "        output.append(array[0])\n",
        "    \n",
        "    return output\n",
        "\n",
        "\n",
        "hs_pred = func(hs)\n",
        "tr_pred = func(tr)\n",
        "ag_pred = func(ag)\n",
        "\n",
        "print(len(hs_pred), len(tr_pred), len(ag_pred))"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "500 500 500\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "2IJCgb0J9Aza",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "\n",
        "with open(\"es_b.tsv\", \"w\") as file:\n",
        "    for i in range(len(hs_pred)):\n",
        "        file.write(str(id_test[i]))\n",
        "        file.write('\\t')\n",
        "        file.write(str(hs_pred[i]))\n",
        "        file.write('\\t')\n",
        "        file.write(str(tr_pred[i]))\n",
        "        file.write('\\t')\n",
        "        file.write(str(ag_pred[i]))\n",
        "        file.write('\\n')\n",
        "    \n",
        "files.download('es_b.tsv')"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}