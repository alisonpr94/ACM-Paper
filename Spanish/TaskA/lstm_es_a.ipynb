{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "lstm_es_a.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "QL28WSHoBsfp",
        "colab_type": "code",
        "outputId": "9350bc06-7c98-44b4-c9a5-73a0a76cbacb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\n",
        "#@author: alison\n",
        "\n",
        "import re\n",
        "import string\n",
        "import pickle\n",
        "import keras\n",
        "import nltk\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer, SnowballStemmer\n",
        "from nltk.tokenize import TweetTokenizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing import sequence\n",
        "from keras.models import Sequential\n",
        "from keras.models import Model\n",
        "from keras.layers import Dense, Dropout, Embedding\n",
        "from keras.layers import LSTM, Activation, Input\n",
        "from keras.optimizers import Adam, SGD, RMSprop\n",
        "from keras import optimizers\n",
        "from keras import regularizers"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "XdekfYsoV2c2",
        "colab_type": "code",
        "outputId": "7279611b-5cc8-407c-c20d-bbd9f231ab36",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        }
      },
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "metadata": {
        "id": "-TpI0dMJEi6p",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!pip install -U -q PyDrive\n",
        " \n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        " \n",
        "# 1. Authenticate and create the PyDrive client.\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "XEfvBz8IEqrO",
        "colab_type": "code",
        "outputId": "977d4ed0-fdec-4847-e31a-ba578b15d0f4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 407
        }
      },
      "cell_type": "code",
      "source": [
        "file_list = drive.ListFile({'q': \"'1Hx5OP1Yrlh37yYzSMtsv6Ui_fuzOuG04' in parents and trashed=false\"}).GetList()\n",
        "for file1 in file_list:\n",
        "    print('title: %s, id: %s' % (file1['title'], file1['id']))"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "title: lstm_es_a.ipynb, id: 1yZRPqV6WKQ8OeNwcfpQwj0jqiIMehAHK\n",
            "title: gru_es_a.ipynb, id: 11DtYTZbYyKHgTFIMXhy8Y4I7c3_0z_W0\n",
            "title: CNN_en_a.ipynb, id: 19oqoK5aiZrBvFOPN1v89xNcXLevjJY0s\n",
            "title: WordEmbedding-biLSTM_en_a.ipynb, id: 1JOBvoH52NYgevXOGS-K5Cw4ggR4VYWZP\n",
            "title: SelfAttention-biLSTM_en_a.ipynb, id: 1wh-yg27fq8OWwO8Jj-Oy47leYf9ujX7U\n",
            "title: WordEmbedding-LSTM_en_a.ipynb, id: 1S_nrY6BmP5Bd6jTGwGLhhQW_W6ZZK6Tf\n",
            "title: WordEmbedding-GRU_en_a.ipynb, id: 1fcw8EGX2Hdz-tTbz0NDEhZlOHiPvKNsY\n",
            "title: bi_lstm_en_a.ipynb, id: 1O-dGg5mtIxPsI4JY16dcp4ytezzN5NQZ\n",
            "title: lstm_en_a.ipynb, id: 1w3Rq-g3l6u5SsBoXoNcKeciE0YUMLJcP\n",
            "title: gru_en_a.ipynb, id: 1qHxV1RB5vy_M889397xa1Ep4kEx_9RIa\n",
            "title: CNN_es_a.ipynb, id: 1UDY2L6Ap8rqV0HtWw9tnLICN4MUD3Nno\n",
            "title: WordEmbedding-biLSTM_es_a.ipynb, id: 14eSqh0xSpcRJXYwU8Qkv9eJaSW_dt3zb\n",
            "title: WordEmbedding-LSTM_es_a.ipynb, id: 16FhVQWB5WvbRXDGkDgsLJWOTqyKwaDwb\n",
            "title: WordEmbedding-GRU_es_a.ipynb, id: 1ahrxlOEINVnCp05yCjmLx8b4v-5SSGL6\n",
            "title: MachineLearning_en.ipynb, id: 1G3f9CvdqZvqLwDQ3kFO3AwgiGEpysIAs\n",
            "title: dev_es.tsv, id: 1EC9OkjN6PR5RqehcYXoci7dsuXmlMA0f\n",
            "title: train_es.tsv, id: 18OJPw3-c99gT1XgKZRXCkI_7xexnBV3z\n",
            "title: trial_es.tsv, id: 1lwuVtwkeYCoBn4wsOEwq1Y1gBe4sazaS\n",
            "title: English.ipynb, id: 1GfiT7bYJ-1LEae10IG1rfhQtSmNZAUVI\n",
            "title: train_en.tsv, id: 1TIjlRkVNIvM8NL3P-4UAmMY6moF3mff0\n",
            "title: dev_en.tsv, id: 1QqOc_95fjvjbw7-uYT37veooqMzfYn-p\n",
            "title: trial_en.tsv, id: 1rQ4h1lQi12lyAo2VL7xuUR5iZ9Bsa4MR\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "h2hLjMJyE3dR",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "train_downloaded = drive.CreateFile({'id': '18OJPw3-c99gT1XgKZRXCkI_7xexnBV3z'})\n",
        "train_downloaded.GetContentFile('train_es.tsv')\n",
        "test_downloaded = drive.CreateFile({'id': '1EC9OkjN6PR5RqehcYXoci7dsuXmlMA0f'})\n",
        "test_downloaded.GetContentFile('dev_es.tsv')\n",
        "trial_downloaded = drive.CreateFile({'id': '1lwuVtwkeYCoBn4wsOEwq1Y1gBe4sazaS'})\n",
        "trial_downloaded.GetContentFile('trial_es.tsv')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "0s3Dnnk8FF4j",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "train = pd.read_csv('train_es.tsv', delimiter='\\t',encoding='utf-8')\n",
        "dev = pd.read_csv('dev_es.tsv', delimiter='\\t',encoding='utf-8')\n",
        "#trial = pd.read_csv('trial_es.tsv', delimiter='\\t',encoding='utf-8')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "lrslbThJCeSY",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Etapa de prÃ©-processamento\n",
        "\n",
        "def clean_tweets(tweet):\n",
        "    tweet = re.sub('@(\\\\w{1,15})\\b', '', tweet)\n",
        "    tweet = tweet.replace(\"via \", \"\")\n",
        "    tweet = tweet.replace(\"RT \", \"\")\n",
        "    tweet = tweet.lower()\n",
        "    return tweet\n",
        "    \n",
        "def clean_url(tweet):\n",
        "    tweet = re.sub('http\\\\S+', '', tweet, flags=re.MULTILINE)   \n",
        "    return tweet\n",
        "    \n",
        "def remove_stop_words(tweet):\n",
        "    stops = set(stopwords.words(\"spanish\"))\n",
        "    stops.update(['.',',','\"',\"'\",'?',':',';','(',')','[',']','{','}'])\n",
        "    toks = [tok for tok in tweet if not tok in stops and len(tok) >= 3]\n",
        "    return toks\n",
        "    \n",
        "def stemming_tweets(tweet):\n",
        "    stemmer = SnowballStemmer('spanish')\n",
        "    stemmed_words = [stemmer.stem(word) for word in tweet]\n",
        "    return stemmed_words\n",
        "\n",
        "def remove_number(tweet):\n",
        "    newTweet = re.sub('\\\\d+', '', tweet)\n",
        "    return newTweet\n",
        "\n",
        "def remove_hashtags(tweet):\n",
        "    result = ''\n",
        "\n",
        "    for word in tweet.split():\n",
        "        if word.startswith('#') or word.startswith('@'):\n",
        "            result += word[1:]\n",
        "            result += ' '\n",
        "        else:\n",
        "            result += word\n",
        "            result += ' '\n",
        "\n",
        "    return result"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "QDsnLDRQGl71",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def preprocessing(tweet, swords = True, url = True, stemming = True, ctweets = True, number = True, hashtag = True):\n",
        "\n",
        "    if ctweets:\n",
        "        tweet = clean_tweets(tweet)\n",
        "\n",
        "    if url:\n",
        "        tweet = clean_url(tweet)\n",
        "\n",
        "    if hashtag:\n",
        "        tweet = remove_hashtags(tweet)\n",
        "    \n",
        "    twtk = TweetTokenizer(strip_handles=True, reduce_len=True)\n",
        "\n",
        "    if number:\n",
        "        tweet = remove_number(tweet)\n",
        "    \n",
        "    tokens = [w.lower() for w in twtk.tokenize(tweet) if w != \"\" and w is not None]\n",
        "\n",
        "    if swords:\n",
        "        tokens = remove_stop_words(tokens)\n",
        "\n",
        "    if stemming:\n",
        "        tokens = stemming_tweets(tokens)\n",
        "\n",
        "    text = \" \".join(tokens)\n",
        "\n",
        "    return text"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "SunHFjyyFLR3",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "train_text = train['text'].map(lambda x: preprocessing(x, swords = True, url = True, stemming = True, ctweets = True, number = True, hashtag = True))\n",
        "y_train    = train['HS']\n",
        "id_train   = train['id']\n",
        "\n",
        "test_text  = dev['text'].map(lambda x: preprocessing(x, swords = True, url = True, stemming = True, ctweets = True, number = True, hashtag = True))\n",
        "y_test     = dev['HS']\n",
        "id_test    = dev['id']\n",
        "\n",
        "data = np.concatenate((train_text, test_text), axis=0)\n",
        "classes = np.concatenate((y_train, y_test), axis=0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "uMBlF6AAryHm",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "max_features = 25000\n",
        "maxlen = 100\n",
        "batch_size = 100\n",
        "epochs = 5\n",
        "\n",
        "# Treina um tokenizaddor nos dados de treino\n",
        "tokenizer = Tokenizer(num_words=max_features)\n",
        "tokenizer.fit_on_texts(data)\n",
        "\n",
        "# Tokeniza os dados\n",
        "X = tokenizer.texts_to_sequences(data)\n",
        "Y = tokenizer.texts_to_sequences(test_text)\n",
        "\n",
        "tweets = sequence.pad_sequences(X, maxlen=maxlen)\n",
        "x_test = sequence.pad_sequences(Y, maxlen=maxlen)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "4-WbNXqRr3LE",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "x_train, x_val, y_train, y_val = train_test_split(tweets, classes, test_size=0.25, random_state=None)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "pTZGv8lcWGIU",
        "colab_type": "code",
        "outputId": "6ecf26e8-3d0a-4c25-bcb3-420d0ff05702",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 567
        }
      },
      "cell_type": "code",
      "source": [
        "# Fase de classificaÃ§Ã£o de sentimentos\n",
        "\n",
        "tweet_input = Input(shape=(maxlen,), dtype='int32')\n",
        "\n",
        "embedding = Embedding(max_features, 256, input_length=maxlen, trainable=True)(tweet_input)\n",
        "\n",
        "gru1 = LSTM(256, activation='tanh', dropout=0.2, recurrent_dropout=0.1, kernel_initializer='normal', return_sequences=True)(embedding)\n",
        "\n",
        "gru2 = LSTM(256, activation='tanh')(gru1)\n",
        "\n",
        "dens = Dense(256, activation='relu')(gru2)\n",
        "\n",
        "output = Dense(1, activation='sigmoid')(dens)\n",
        "\n",
        "model = Model(inputs=tweet_input, outputs=output)\n",
        "\n",
        "#opt = Adam(lr=0.001, beta_1=0.9, beta_2=0.825, epsilon=1e-08)\n",
        "#opt = SGD(lr=0.01, momentum=0.0, decay=0.0, nesterov=False)\n",
        "opt = RMSprop(lr=0.001, rho=0.9, epsilon=None, decay=0.01)\n",
        "\n",
        "model.compile(optimizer=opt, loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "model.summary()\n",
        "\n",
        "model.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, shuffle=True, validation_data=(x_val, y_val))\n",
        "\n",
        "y_pred = (model.predict(x_test, batch_size=batch_size, verbose=1) > .5).astype(int)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_3 (InputLayer)         (None, 100)               0         \n",
            "_________________________________________________________________\n",
            "embedding_3 (Embedding)      (None, 100, 256)          6400000   \n",
            "_________________________________________________________________\n",
            "lstm_5 (LSTM)                (None, 100, 256)          525312    \n",
            "_________________________________________________________________\n",
            "lstm_6 (LSTM)                (None, 256)               525312    \n",
            "_________________________________________________________________\n",
            "dense_5 (Dense)              (None, 256)               65792     \n",
            "_________________________________________________________________\n",
            "dense_6 (Dense)              (None, 1)                 257       \n",
            "=================================================================\n",
            "Total params: 7,516,673\n",
            "Trainable params: 7,516,673\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Train on 3726 samples, validate on 1243 samples\n",
            "Epoch 1/5\n",
            "3726/3726 [==============================] - 21s 6ms/step - loss: 0.6689 - acc: 0.6484 - val_loss: 0.6751 - val_acc: 0.6307\n",
            "Epoch 2/5\n",
            "3726/3726 [==============================] - 19s 5ms/step - loss: 0.4205 - acc: 0.8191 - val_loss: 0.5300 - val_acc: 0.7530\n",
            "Epoch 3/5\n",
            "3726/3726 [==============================] - 19s 5ms/step - loss: 0.2909 - acc: 0.8873 - val_loss: 0.5500 - val_acc: 0.7273\n",
            "Epoch 4/5\n",
            "3726/3726 [==============================] - 19s 5ms/step - loss: 0.2179 - acc: 0.9219 - val_loss: 0.6278 - val_acc: 0.7273\n",
            "Epoch 5/5\n",
            "3726/3726 [==============================] - 19s 5ms/step - loss: 0.1536 - acc: 0.9450 - val_loss: 0.7550 - val_acc: 0.7184\n",
            "500/500 [==============================] - 1s 3ms/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "rx8QHLaNWgdn",
        "colab_type": "code",
        "outputId": "b09d97a4-b0ab-4137-cfbd-1ee4513ecf8a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        }
      },
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import f1_score, precision_score, accuracy_score, recall_score\n",
        "\n",
        "print(\"F1.........: %f\" %(f1_score(y_test, y_pred, average=\"macro\")))\n",
        "print(\"Precision..: %f\" %(precision_score(y_test, y_pred, average=\"macro\")))\n",
        "print(\"Recall.....: %f\" %(recall_score(y_test, y_pred, average=\"macro\")))\n",
        "print(\"Accuracy...: %f\" %(accuracy_score(y_test, y_pred)))"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "F1.........: 0.881036\n",
            "Precision..: 0.879949\n",
            "Recall.....: 0.882996\n",
            "Accuracy...: 0.882000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "o2oDoSc55nxN",
        "colab_type": "code",
        "outputId": "eb869f5d-b350-4fee-f8d9-edd1ee6d5b2a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "output = []\n",
        "for array in y_pred:\n",
        "    output.append(array[0])\n",
        "print(len(output), len(id_test))"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "500 500\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "H6MN2vcq5OyW",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "\n",
        "with open(\"es_a.tsv\", \"w\") as file:\n",
        "    for i in range(len(y_pred)):\n",
        "        file.write(str(id_test[i]))\n",
        "        file.write('\\t')\n",
        "        file.write(str(output[i]))\n",
        "        file.write('\\n')\n",
        "\n",
        "files.download('es_a.tsv')"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}